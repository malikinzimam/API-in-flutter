{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PdKwzEluDBN7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install openai-agents SDK"
      ],
      "metadata": {
        "id": "PdKwzEluDBN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents  \"openai-agents[litellm]\""
      ],
      "metadata": {
        "id": "3QdkOviEB2ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7208e7be-2eeb-4c95-8ef6-d10a3d7e99bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.8/116.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make your Jupyter Notebook capable of running asynchronous functions."
      ],
      "metadata": {
        "id": "7yD91lz4DIAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "7A5YLi3HCfBV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Google Gemini with LiteLLm and OPENAI-Agent SDK"
      ],
      "metadata": {
        "id": "K3VTUWDaGFcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Run Sync"
      ],
      "metadata": {
        "id": "P_zEYQa1kHC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(model: str, api_key: str):\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in AI.\",\n",
        "      model=LitellmModel(model=MODEL, api_key=api_key),\n",
        "\n",
        "  )\n",
        "\n",
        "  result = Runner.run_sync(agent, \"weather in karachi\")\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "main(model=MODEL, api_key=GEMINI_API_KEY)\n",
        "\n"
      ],
      "metadata": {
        "id": "WBT9Z8hE6kEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5772ce21-69a7-46b8-885f-f760cf6b43f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karachi is currently 32°C and mostly clear. The wind is blowing from the southwest at 18 km/h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Async Function"
      ],
      "metadata": {
        "id": "S7ECiU-f5BAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tool celling"
      ],
      "metadata": {
        "id": "kAnUqei0vFwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str)->str:\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "\n",
        "async def main(model: str, api_key: str):\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in haikus.\",\n",
        "      model=LitellmModel(model=model, api_key=api_key),\n",
        "      tools=[get_weather]\n",
        "\n",
        "  )\n",
        "\n",
        "  result = await Runner.run(agent, \"What's the weather in karachi?\")\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "asyncio.run(main(model=MODEL, api_key=GEMINI_API_KEY))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-QbKRj7wSzr",
        "outputId": "943f62dc-8398-45d5-e0cb-32572eb93758"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[debug] getting weather for karachi\n",
            "The sun shines brightly,\n",
            "Karachi feels the warm rays,\n",
            "A sunny weather.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***handoofs***\n"
      ],
      "metadata": {
        "id": "Hc6ZY9xsvR5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Await\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str)->str:\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "@function_tool\n",
        "def get_ai():\n",
        "  return \"this is ai\"\n",
        "\n",
        "\n",
        "weather_agent = Agent(\n",
        "  name=\"WeatherAssistant\",\n",
        "  instructions=\"You only respond in weather.\",\n",
        "  model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "  handoff_description=\"weather assistant\",\n",
        "  tools=[get_weather]\n",
        "\n",
        ")\n",
        "ai_agent = Agent(\n",
        "  name=\"AiAssistant\",\n",
        "  instructions=\"You only respond in AI information.\",\n",
        "  model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "  handoff_description=\"ai assistant\",\n",
        "  tools=[get_ai]\n",
        ")\n",
        "\n",
        "agent = Agent(\n",
        "  name=\"Assistant\",\n",
        "  instructions=\"You only respond in haikus.\",\n",
        "  model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "  handoffs=[weather_agent,ai_agent]\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "result = await Runner.run(agent,\"what is a ai\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "id": "-OvK5V-0va0O",
        "outputId": "fb44b85c-8fe9-4709-9a76-82643c28f53f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can't define that.\n",
            "I can get ai help, though,\n",
            "If you would like that.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.last_agent.name"
      ],
      "metadata": {
        "id": "315jPY-00KE3",
        "outputId": "bdcd40b2-3b3b-4d74-bcab-d67090bdad7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Assistant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "# Disable tracing\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "# Model and API Key\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Tool: Weather\n",
        "@function_tool\n",
        "def get_weather(city: str) -> str:\n",
        "    print(f\"[debug] Getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "# Tool: AI Info\n",
        "@function_tool\n",
        "def get_ai() -> str:\n",
        "    return \"This is AI, the simulation of human intelligence in machines.\"\n",
        "\n",
        "# Sub-agent: WeatherAssistant\n",
        "weather_agent = Agent(\n",
        "    name=\"WeatherAssistant\",\n",
        "    instructions=\"You only respond to weather-related questions. Use the get_weather tool.\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    handoff_description=\"Answers weather-related questions\",\n",
        "    tools=[get_weather]\n",
        ")\n",
        "\n",
        "# Sub-agent: AiAssistant\n",
        "ai_agent = Agent(\n",
        "    name=\"AiAssistant\",\n",
        "    instructions=\"You only respond to AI-related questions. Use the get_ai tool.\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    handoff_description=\"Answers AI-related questions\",\n",
        "    tools=[get_ai]\n",
        ")\n",
        "\n",
        "# Main Agent: Decides whether to handoff to weather_agent or ai_agent\n",
        "agent = Agent(\n",
        "    name=\"SmartAssistant\",\n",
        "    instructions=\"If the question is about weather, hand it off to WeatherAssistant. If it's about AI, hand it off to AiAssistant.\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    handoffs=[weather_agent, ai_agent]\n",
        ")\n",
        "\n",
        "result = await Runner.run(agent,\"what is the weather is karachi\")\n",
        "print(result.final_output)\n",
        "print(result.last_agent.name)\n",
        "\n",
        "\n",
        "result = await Runner.run(agent,\"what is a ai\")\n",
        "print(result.final_output)\n",
        "print(result.last_agent.name)\n",
        "\n",
        "result = await Runner.run(agent,\"hi\")\n",
        "print(result.final_output)\n",
        "print(result.last_agent.name)\n"
      ],
      "metadata": {
        "id": "xpeTDSVJ1vB9",
        "outputId": "498d04b0-4c1f-4296-fa9d-15a6047173dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[debug] Getting weather for karachi\n",
            "The weather in karachi is sunny.\n",
            "\n",
            "WeatherAssistant\n",
            "I am an AI assistant. How can I help you today?\n",
            "\n",
            "AiAssistant\n",
            "How can I help you today?\n",
            "\n",
            "SmartAssistant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Await  # This line is incorrect and not needed\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "# Disable tracing for performance or debugging\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "# Model & API Key\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# ----------- Tools -----------\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str) -> str:\n",
        "    print(f\"[debug] Getting weather for {city}\")\n",
        "    return f\"The weather in {city} is currently sunny with a mild breeze.\"\n",
        "\n",
        "@function_tool\n",
        "def get_ai() -> str:\n",
        "    return \"AI (Artificial Intelligence) refers to the simulation of human intelligence by machines.\"\n",
        "\n",
        "# ----------- Sub Agents -----------\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"WeatherAssistant\",\n",
        "    instructions=(\n",
        "        \"You are a weather assistant. \"\n",
        "        \"Only provide accurate and concise weather-related responses. \"\n",
        "        \"Do not answer anything unrelated to weather.\"\n",
        "    ),\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    handoff_description=\"Handles weather-related queries\",\n",
        "    tools=[get_weather]\n",
        ")\n",
        "\n",
        "ai_agent = Agent(\n",
        "    name=\"AiAssistant\",\n",
        "    instructions=(\n",
        "        \"You are an AI information expert. \"\n",
        "        \"Only respond to questions about Artificial Intelligence in a clear and informative way.\"\n",
        "    ),\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    handoff_description=\"Handles AI-related queries\",\n",
        "    tools=[get_ai]\n",
        ")\n",
        "\n",
        "# ----------- Main Agent -----------\n",
        "\n",
        "main_agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You are a poetic assistant who speaks in haikus. \"\n",
        "        \"If the user query is about weather or AI, hand it off to the appropriate specialized agent.\"\n",
        "    ),\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    handoffs=[weather_agent, ai_agent]\n",
        ")\n",
        "\n",
        "# ----------- Run the Agent -----------\n",
        "\n",
        "result = await Runner.run(agent,\"what is the weather is karachi\")\n",
        "print(result.final_output)\n",
        "print(result.last_agent.name)\n",
        "\n",
        "\n",
        "result = await Runner.run(agent,\"ai kia ha or isa kasa sikha\")\n",
        "print(result.final_output)\n",
        "print(result.last_agent.name)\n",
        "\n",
        "result = await Runner.run(agent,\"hi\")\n",
        "print(result.final_output)\n",
        "print(result.last_agent.name)"
      ],
      "metadata": {
        "id": "27SDLURe4lAk",
        "outputId": "8a86c0f9-e077-42b3-838b-4e91cf9f686d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To weather assistant,\n",
            "I must send your simple quest.\n",
            "Awaiting results.\n",
            "\n",
            "Assistant\n",
            "To AI assistant,\n",
            "I must pass your query now,\n",
            "For its deep wisdom.\n",
            "\n",
            "Assistant\n",
            "A greeting to you,\n",
            "Words soft as the falling leaves,\n",
            "May peace fill your day.\n",
            "\n",
            "Assistant\n"
          ]
        }
      ]
    }
  ]
}